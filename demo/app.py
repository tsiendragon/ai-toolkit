#!/usr/bin/env python3
"""
FLUX Kontext LoRA Web Demo
æ”¯æŒFLUX Kontext Devæ¨¡å‹çš„LoRA checkpointåŠ è½½å’Œæ¨ç†æ¼”ç¤º
"""

import os
import sys
import json
import random
import asyncio
import tempfile
from pathlib import Path
from typing import List, Optional, Dict, Any
from contextlib import asynccontextmanager

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import uvicorn
from fastapi import FastAPI, File, UploadFile, HTTPException, Request, Form
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from PIL import Image

# AI Toolkit imports
from toolkit.config_modules import ModelConfig, DatasetConfig, SaveConfig, NetworkConfig, GenerateImageConfig
from extensions_built_in.diffusion_models.flux_kontext.flux_kontext import FluxKontextModel
from toolkit.assistant_lora import load_assistant_lora_from_path
from toolkit.train_tools import get_torch_dtype


def flush():
    """æ¸…ç†GPUå†…å­˜"""
    torch.cuda.empty_cache()
    import gc
    gc.collect()


class AppState:
    """åº”ç”¨çŠ¶æ€ç®¡ç†"""
    def __init__(self):
        self.model: Optional[FluxKontextModel] = None
        self.lora_network = None
        self.current_lora_path: Optional[str] = None
        self.is_model_loaded = False
        self.device = None
        self.torch_dtype = None

    def reset(self):
        """é‡ç½®çŠ¶æ€"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.lora_network is not None:
            del self.lora_network
            self.lora_network = None
        self.current_lora_path = None
        self.is_model_loaded = False
        torch.cuda.empty_cache()
        flush()


# å…¨å±€åº”ç”¨çŠ¶æ€
app_state = AppState()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    app_state.device = "cuda" if torch.cuda.is_available() else "cpu"
    app_state.torch_dtype = get_torch_dtype("bf16")

    print(f"ğŸš€ å¯åŠ¨FLUX Kontext Demo")
    print(f"ğŸ“± è®¾å¤‡: {app_state.device}")
    print(f"ğŸ”¢ æ•°æ®ç±»å‹: {app_state.torch_dtype}")

    yield

    # å…³é—­æ—¶æ¸…ç†
    print("ğŸ§¹ æ¸…ç†èµ„æº...")
    app_state.reset()


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="FLUX Kontext LoRA Demo",
    description="FLUX Kontext Devæ¨¡å‹LoRA checkpointåŠ è½½å’Œæ¨ç†æ¼”ç¤º",
    version="1.0.0",
    lifespan=lifespan
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æŒ‚è½½é™æ€æ–‡ä»¶
demo_path = Path(__file__).parent
app.mount("/static", StaticFiles(directory=demo_path / "static"), name="static")

# æŒ‚è½½æ ·æœ¬è¾“å‡ºç›®å½•
SAMPLES_OUTPUT_DIR = Path("/data/lilong/flux_kyc/samples")
SAMPLES_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
app.mount("/samples", StaticFiles(directory=SAMPLES_OUTPUT_DIR), name="samples")


# Pydanticæ¨¡å‹
class ModelStatus(BaseModel):
    is_loaded: bool = False
    current_lora_path: Optional[str] = None
    device: Optional[str] = None
    memory_info: Optional[Dict[str, Any]] = None


class LoadModelRequest(BaseModel):
    lora_path: str = Field(
        default="/data/lilong/experiment/id_card_flux_kontext_lora_v1/id_card_flux_kontext_lora_v1/id_card_flux_kontext_lora_v1_000001111.safetensors",
        description="LoRA checkpointè·¯å¾„"
    )


class BatchInferenceRequest(BaseModel):
    folder_path: str = Field(
        default="/home/sysop/data/id_card/training_images",
        description="å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„"
    )
    max_samples: int = Field(default=5, ge=1, le=20, description="æœ€å¤§é‡‡æ ·æ•°é‡")
    seed: int = Field(default=42, description="éšæœºç§å­")
    guidance_scale: float = Field(default=4.0, ge=1.0, le=20.0, description="å¼•å¯¼å¼ºåº¦")
    num_inference_steps: int = Field(default=20, ge=10, le=50, description="æ¨ç†æ­¥æ•°")


class SingleInferenceRequest(BaseModel):
    prompt: str = Field(..., description="æ–‡æœ¬æç¤º")
    control_image_path: Optional[str] = Field(None, description="æ§åˆ¶å›¾åƒè·¯å¾„")
    width: int = Field(default=832, ge=512, le=1024, description="å›¾åƒå®½åº¦")
    height: int = Field(default=576, ge=512, le=1024, description="å›¾åƒé«˜åº¦")
    seed: int = Field(default=42, description="éšæœºç§å­")
    guidance_scale: float = Field(default=4.0, ge=1.0, le=20.0, description="å¼•å¯¼å¼ºåº¦")
    num_inference_steps: int = Field(default=20, ge=10, le=50, description="æ¨ç†æ­¥æ•°")


def get_memory_info():
    """è·å–GPUå†…å­˜ä¿¡æ¯"""
    if torch.cuda.is_available():
        return {
            "allocated": f"{torch.cuda.memory_allocated() / 1024**3:.2f} GB",
            "reserved": f"{torch.cuda.memory_reserved() / 1024**3:.2f} GB",
            "max_allocated": f"{torch.cuda.max_memory_allocated() / 1024**3:.2f} GB"
        }
    return None


@app.get("/", response_class=HTMLResponse)
async def root():
    """ä¸»é¡µé¢"""
    html_file = demo_path / "index.html"
    if html_file.exists():
        return FileResponse(html_file)
    else:
        return HTMLResponse("""
        <html><body>
        <h1>FLUX Kontext LoRA Demo</h1>
        <p>è¯·å…ˆåˆ›å»º index.html æ–‡ä»¶</p>
        </body></html>
        """)


@app.get("/api/status", response_model=ModelStatus)
async def get_status():
    """è·å–æ¨¡å‹çŠ¶æ€"""
    return ModelStatus(
        is_loaded=app_state.is_model_loaded,
        current_lora_path=app_state.current_lora_path,
        device=str(app_state.device) if app_state.device else None,
        memory_info=get_memory_info()
    )


@app.post("/api/load_model")
async def load_model(request: LoadModelRequest):
    """åŠ è½½æ¨¡å‹å’ŒLoRA"""
    try:
        # å¦‚æœå·²ç»åŠ è½½äº†ç›¸åŒçš„LoRAï¼Œç›´æ¥è¿”å›
        if (app_state.is_model_loaded and
            app_state.current_lora_path == request.lora_path):
            return {
                "success": True,
                "message": f"æ¨¡å‹å·²åŠ è½½: {request.lora_path}",
                "status": await get_status()
            }

        # æ¸…ç†ä¹‹å‰çš„çŠ¶æ€
        app_state.reset()

        # éªŒè¯LoRAæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(request.lora_path):
            raise HTTPException(status_code=404, detail=f"LoRAæ–‡ä»¶ä¸å­˜åœ¨: {request.lora_path}")

        print(f"ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹å’ŒLoRA: {request.lora_path}")

        # åˆ›å»ºæ¨¡å‹é…ç½®
        model_config = ModelConfig(
            name_or_path="black-forest-labs/FLUX.1-Kontext-dev",
            arch="flux_kontext",
            quantize=True,
            low_vram=True,
            inference_lora_path=request.lora_path
        )

        # åˆ›å»ºå¹¶åŠ è½½æ¨¡å‹
        app_state.model = FluxKontextModel(
            device=app_state.device,
            model_config=model_config,
            dtype="bf16"
        )

        print("ğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹...")
        app_state.model.load_model()

        print("ğŸ”— åŠ è½½LoRA...")
        # FluxKontextModelç»§æ‰¿è‡ªBaseModelï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’æ¨¡å‹æœ¬èº«
        app_state.lora_network = load_assistant_lora_from_path(
            request.lora_path,
            app_state.model
        )

        app_state.current_lora_path = request.lora_path
        app_state.is_model_loaded = True

        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")

        return {
            "success": True,
            "message": f"æ¨¡å‹åŠ è½½æˆåŠŸ: {os.path.basename(request.lora_path)}",
            "status": await get_status()
        }

    except Exception as e:
        app_state.reset()
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")


@app.post("/api/unload_model")
async def unload_model():
    """å¸è½½æ¨¡å‹"""
    try:
        if not app_state.is_model_loaded:
            return {
                "success": True,
                "message": "æ¨¡å‹æœªåŠ è½½",
                "status": await get_status()
            }

        print("ğŸ—‘ï¸ å¸è½½æ¨¡å‹...")
        app_state.reset()

        print("âœ… æ¨¡å‹å¸è½½å®Œæˆ!")

        return {
            "success": True,
            "message": "æ¨¡å‹å¸è½½æˆåŠŸ",
            "status": await get_status()
        }

    except Exception as e:
        print(f"âŒ æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")


@app.post("/api/batch_inference")
async def batch_inference(request: BatchInferenceRequest):
    """æ‰¹é‡æ¨ç†"""
    if not app_state.is_model_loaded:
        raise HTTPException(status_code=400, detail="è¯·å…ˆåŠ è½½æ¨¡å‹")

    try:
        # éªŒè¯æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(request.folder_path):
            raise HTTPException(status_code=404, detail=f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {request.folder_path}")

        # æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
        image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png']:
            image_files.extend(Path(request.folder_path).glob(ext))

        if not image_files:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")

        # éšæœºé‡‡æ ·
        random.seed(request.seed)
        selected_files = random.sample(image_files, min(request.max_samples, len(image_files)))

        results = []
        for i, img_path in enumerate(selected_files):
            print(f"ğŸ–¼ï¸ å¤„ç†å›¾åƒ {i+1}/{len(selected_files)}: {img_path.name}")

            # æŸ¥æ‰¾å¯¹åº”çš„æç¤ºæ–‡ä»¶
            txt_path = img_path.with_suffix('.txt')
            prompt = ""
            if txt_path.exists():
                prompt = txt_path.read_text(encoding='utf-8').strip()

            # TODO: è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„æ¨ç†é€»è¾‘
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            output_filename = f"batch_{request.seed}_{i:04d}.jpg"
            output_path = SAMPLES_OUTPUT_DIR / output_filename

            # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ - å®é™…å®ç°æ—¶è¿™é‡Œä¼šè°ƒç”¨æ¨¡å‹æ¨ç†
            results.append({
                "original_image": str(img_path),
                "prompt": prompt,
                "generated_image": f"/samples/{output_filename}",
                "output_path": str(output_path),
                "success": True
            })

        return {
            "success": True,
            "message": f"æ‰¹é‡æ¨ç†å®Œæˆï¼Œå¤„ç†äº† {len(results)} å¼ å›¾åƒ",
            "results": results
        }

    except Exception as e:
        print(f"âŒ æ‰¹é‡æ¨ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡æ¨ç†å¤±è´¥: {str(e)}")


@app.post("/api/single_inference")
async def single_inference(request: SingleInferenceRequest):
    """å•ä¸ªæ¨ç†"""
    if not app_state.is_model_loaded:
        raise HTTPException(status_code=400, detail="è¯·å…ˆåŠ è½½æ¨¡å‹")

    try:
        print(f"ğŸ¨ å¼€å§‹å•ä¸ªæ¨ç†: {request.prompt[:50]}...")

        # TODO: è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„æ¨ç†é€»è¾‘
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        import time
        timestamp = int(time.time())
        output_filename = f"single_{timestamp}_{request.seed}.jpg"
        output_path = SAMPLES_OUTPUT_DIR / output_filename
        output_url = f"/samples/{output_filename}"

        return {
            "success": True,
            "message": "æ¨ç†å®Œæˆ",
            "generated_image": output_url,
            "prompt": request.prompt,
            "parameters": {
                "width": request.width,
                "height": request.height,
                "guidance_scale": request.guidance_scale,
                "num_inference_steps": request.num_inference_steps,
                "seed": request.seed
            }
        }

    except Exception as e:
        print(f"âŒ å•ä¸ªæ¨ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å•ä¸ªæ¨ç†å¤±è´¥: {str(e)}")


@app.post("/api/upload_image")
async def upload_image(file: UploadFile = File(...)):
    """ä¸Šä¼ å›¾åƒ"""
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not file.content_type.startswith("image/"):
            raise HTTPException(status_code=400, detail="è¯·ä¸Šä¼ å›¾åƒæ–‡ä»¶")

        # åˆ›å»ºä¸Šä¼ ç›®å½•ï¼ˆä¿å­˜åˆ°æ ·æœ¬ç›®å½•ä¸‹çš„uploadså­ç›®å½•ï¼‰
        upload_dir = SAMPLES_OUTPUT_DIR / "uploads"
        upload_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åé¿å…å†²çª
        import time
        timestamp = int(time.time())
        filename = f"{timestamp}_{file.filename}"
        file_path = upload_dir / filename

        # ä¿å­˜æ–‡ä»¶
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)

        return {
            "success": True,
            "message": "å›¾åƒä¸Šä¼ æˆåŠŸ",
            "file_path": f"/samples/uploads/{filename}",
            "local_path": str(file_path)
        }

    except Exception as e:
        print(f"âŒ å›¾åƒä¸Šä¼ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å›¾åƒä¸Šä¼ å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        reload_dirs=[str(demo_path)]
    )
