"""
GPUæ¨¡å—ç›‘æ§å·¥å…·
ç”¨äºç›‘æ§å’Œè®°å½•å“ªäº›AIæ¨¡å—å½“å‰åŠ è½½åœ¨GPUä¸Š
Created by Tsien at 2025-01-27
"""

import torch
from typing import Dict, Union, Optional, TYPE_CHECKING

if TYPE_CHECKING:
    from toolkit.stable_diffusion_model import StableDiffusion

# GPUæ¨¡å—ç¼–ç ç³»ç»Ÿ - ä½¿ç”¨ä½æ ‡å¿—ä¾¿äºç»„åˆ
GPU_MODULE_CODES = {
    'VAE_ENCODER': 1,      # VAEç¼–ç å™¨
    'VAE_DECODER': 2,      # VAEè§£ç å™¨
    'UNET': 4,             # UNetä¸»æ¨¡å‹
    'TEXT_ENCODER': 8,     # æ–‡æœ¬ç¼–ç å™¨
    'ADAPTER': 16,         # é€‚é…å™¨æ¨¡å—
    'REFINER_UNET': 32,    # ç²¾ç‚¼å™¨UNet
    'CLIP': 64,            # CLIPæ¨¡å‹
    'OTHER': 128,          # å…¶ä»–æ¨¡å—
}

# æ¨¡å—åç§°æ˜ å°„
MODULE_NAMES = {
    1: 'VAE_ENC',
    2: 'VAE_DEC',
    4: 'UNET',
    8: 'TEXT_ENC',
    16: 'ADAPTER',
    32: 'REFINER',
    64: 'CLIP',
    128: 'OTHER',
}

def get_module_on_gpu(module: torch.nn.Module, target_device: Union[str, torch.device] = 'cuda') -> bool:
    """
    æ£€æŸ¥æ¨¡å—æ˜¯å¦åœ¨æŒ‡å®šGPUè®¾å¤‡ä¸Š

    Args:
        module: PyTorchæ¨¡å—
        target_device: ç›®æ ‡è®¾å¤‡ï¼Œé»˜è®¤ä¸º'cuda'

    Returns:
        bool: æ¨¡å—æ˜¯å¦åœ¨ç›®æ ‡è®¾å¤‡ä¸Š
    """
    if module is None:
        return False

    # æ£€æŸ¥æ¨¡å—çš„ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡
    try:
        for param in module.parameters():
            device_str = str(param.device)
            if isinstance(target_device, torch.device):
                target_device = str(target_device)
            return target_device in device_str
    except:
        return False

    return False

def get_gpu_module_state(model_instance) -> int:
    """
    è·å–å½“å‰GPUä¸ŠåŠ è½½çš„æ¨¡å—çŠ¶æ€ç¼–ç 

    Args:
        model_instance: æ¨¡å‹å®ä¾‹ï¼Œå¯ä»¥æ˜¯StableDiffusionã€VAEè®­ç»ƒæˆ–ESRGANè®­ç»ƒå®ä¾‹

    Returns:
        int: æ¨¡å—çŠ¶æ€ç¼–ç ï¼ˆä½æ ‡å¿—ç»„åˆï¼‰
    """
    state_code = 0

    # æ£€æŸ¥StableDiffusionæ¨¡å‹
    if hasattr(model_instance, 'sd') and model_instance.sd is not None:
        sd = model_instance.sd

        # æ£€æŸ¥VAE (é€šå¸¸VAE encoderå’Œdecoderæ˜¯ä¸€ä½“çš„)
        if hasattr(sd, 'vae') and sd.vae is not None:
            if get_module_on_gpu(sd.vae):
                # VAEé€šå¸¸åŒ…å«encoderå’Œdecoderï¼Œè¿™é‡Œç®€åŒ–ä¸ºåŒæ—¶æ ‡è®°
                state_code |= GPU_MODULE_CODES['VAE_ENCODER']
                state_code |= GPU_MODULE_CODES['VAE_DECODER']

        # æ£€æŸ¥UNet
        if hasattr(sd, 'unet') and sd.unet is not None:
            if get_module_on_gpu(sd.unet):
                state_code |= GPU_MODULE_CODES['UNET']

        # æ£€æŸ¥Text Encoder
        if hasattr(sd, 'text_encoder') and sd.text_encoder is not None:
            # å¤„ç†å•ä¸ªæˆ–å¤šä¸ªtext encoder
            if isinstance(sd.text_encoder, list):
                for te in sd.text_encoder:
                    if get_module_on_gpu(te):
                        state_code |= GPU_MODULE_CODES['TEXT_ENCODER']
                        break
            else:
                if get_module_on_gpu(sd.text_encoder):
                    state_code |= GPU_MODULE_CODES['TEXT_ENCODER']

        # æ£€æŸ¥Adapter
        if hasattr(sd, 'adapter') and sd.adapter is not None:
            if get_module_on_gpu(sd.adapter):
                state_code |= GPU_MODULE_CODES['ADAPTER']

        # æ£€æŸ¥Refiner UNet
        if hasattr(sd, 'refiner_unet') and sd.refiner_unet is not None:
            if get_module_on_gpu(sd.refiner_unet):
                state_code |= GPU_MODULE_CODES['REFINER_UNET']

        # æ£€æŸ¥CLIP (å¦‚æœå•ç‹¬å­˜åœ¨)
        if hasattr(sd, 'clip') and sd.clip is not None:
            if get_module_on_gpu(sd.clip):
                state_code |= GPU_MODULE_CODES['CLIP']

    # æ£€æŸ¥ç‹¬ç«‹çš„VAEæ¨¡å‹ (TrainVAEProcess)
    elif hasattr(model_instance, 'vae') and model_instance.vae is not None:
        if get_module_on_gpu(model_instance.vae):
            state_code |= GPU_MODULE_CODES['VAE_ENCODER']
            state_code |= GPU_MODULE_CODES['VAE_DECODER']

        # æ£€æŸ¥ç›®æ ‡VAE
        if hasattr(model_instance, 'target_latent_vae') and model_instance.target_latent_vae is not None:
            if get_module_on_gpu(model_instance.target_latent_vae):
                state_code |= GPU_MODULE_CODES['OTHER']  # æ ‡è®°ä¸ºå…¶ä»–æ¨¡å—

    # æ£€æŸ¥ESRGANæ¨¡å‹ (TrainESRGANProcess)
    elif hasattr(model_instance, 'model') and model_instance.model is not None:
        if get_module_on_gpu(model_instance.model):
            state_code |= GPU_MODULE_CODES['OTHER']  # ESRGANæ ‡è®°ä¸ºå…¶ä»–æ¨¡å—

        # æ£€æŸ¥åˆ¤åˆ«å™¨
        if hasattr(model_instance, 'critic') and model_instance.critic is not None:
            critic_model = getattr(model_instance.critic, 'critic', None)
            if critic_model is not None and get_module_on_gpu(critic_model):
                state_code |= GPU_MODULE_CODES['ADAPTER']  # åˆ¤åˆ«å™¨æ ‡è®°ä¸ºadapter

    return state_code

def decode_gpu_module_state(state_code: int) -> str:
    """
    å°†æ¨¡å—çŠ¶æ€ç¼–ç è§£ç ä¸ºå¯è¯»å­—ç¬¦ä¸²

    Args:
        state_code: æ¨¡å—çŠ¶æ€ç¼–ç 

    Returns:
        str: å¯è¯»çš„æ¨¡å—çŠ¶æ€æè¿°
    """
    if state_code == 0:
        return "NONE"

    active_modules = []
    for code, name in MODULE_NAMES.items():
        if state_code & code:
            active_modules.append(name)

    return "+".join(active_modules)

def log_gpu_module_state_to_tensorboard(
    writer,
    model_instance,
    step: int,
    tag_prefix: str = "gpu_modules"
) -> None:
    """
    å°†GPUæ¨¡å—çŠ¶æ€è®°å½•åˆ°TensorBoard

    Args:
        writer: TensorBoard SummaryWriter
        model_instance: æ¨¡å‹å®ä¾‹ï¼Œå¯ä»¥æ˜¯StableDiffusionã€VAEè®­ç»ƒæˆ–ESRGANè®­ç»ƒå®ä¾‹
        step: å½“å‰è®­ç»ƒæ­¥æ•°
        tag_prefix: TensorBoardæ ‡ç­¾å‰ç¼€
    """
    if writer is None:
        return

    try:
        # è·å–æ¨¡å—çŠ¶æ€ç¼–ç 
        state_code = get_gpu_module_state(model_instance)

        # è®°å½•æ•°å€¼ç¼–ç 
        writer.add_scalar(f"{tag_prefix}/state_code", state_code, step)

        # è®°å½•æ¯ä¸ªæ¨¡å—çš„çŠ¶æ€
        for code, name in MODULE_NAMES.items():
            module_active = 1 if (state_code & code) else 0
            writer.add_scalar(f"{tag_prefix}/module_{name.lower()}", module_active, step)

        # æ·»åŠ æ–‡æœ¬æè¿° (å¯é€‰)
        state_desc = decode_gpu_module_state(state_code)
        writer.add_text(f"{tag_prefix}/description", state_desc, step)

    except Exception as e:
        print(f"âš ï¸ [GPU_MODULE_MONITOR] è®°å½•GPUæ¨¡å—çŠ¶æ€å¤±è´¥: {e}")

def get_gpu_memory_info() -> Dict[str, float]:
    """
    è·å–GPUå†…å­˜ä½¿ç”¨ä¿¡æ¯

    Returns:
        Dict[str, float]: GPUå†…å­˜ä¿¡æ¯
    """
    if not torch.cuda.is_available():
        return {"used_mb": 0, "total_mb": 0, "free_mb": 0, "utilization": 0}

    try:
        used_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024  # MB
        free_memory = total_memory - used_memory
        utilization = (used_memory / total_memory) * 100

        return {
            "used_mb": used_memory,
            "total_mb": total_memory,
            "free_mb": free_memory,
            "utilization": utilization
        }
    except:
        return {"used_mb": 0, "total_mb": 0, "free_mb": 0, "utilization": 0}

def log_gpu_memory_to_tensorboard(
    writer,
    step: int,
    tag_prefix: str = "gpu_memory"
) -> None:
    """
    å°†GPUå†…å­˜ä½¿ç”¨æƒ…å†µè®°å½•åˆ°TensorBoard

    Args:
        writer: TensorBoard SummaryWriter
        step: å½“å‰è®­ç»ƒæ­¥æ•°
        tag_prefix: TensorBoardæ ‡ç­¾å‰ç¼€
    """
    if writer is None:
        return

    try:
        memory_info = get_gpu_memory_info()

        for key, value in memory_info.items():
            writer.add_scalar(f"{tag_prefix}/{key}", value, step)

    except Exception as e:
        print(f"âš ï¸ [GPU_MODULE_MONITOR] è®°å½•GPUå†…å­˜çŠ¶æ€å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹å’Œè¯´æ˜
"""
ä½¿ç”¨ç¤ºä¾‹:

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
state_code = get_gpu_module_state(self)  # ä¼ é€’è®­ç»ƒå®ä¾‹
print(f"å½“å‰GPUæ¨¡å—: {decode_gpu_module_state(state_code)} (ç¼–ç : {state_code})")

# è®°å½•åˆ°TensorBoard (å·²è‡ªåŠ¨é›†æˆåˆ°è®­ç»ƒæµç¨‹ä¸­)
log_gpu_module_state_to_tensorboard(self.writer, self, self.step_num)
log_gpu_memory_to_tensorboard(self.writer, self.step_num)

æ¨¡å—ç¼–ç è¯´æ˜ (ä½æ ‡å¿—ï¼Œå¯ç»„åˆ):
- 0: æ— æ¨¡å—åœ¨GPU
- 1: VAE Encoder (01)
- 2: VAE Decoder (10)
- 3: VAEå®Œæ•´ (11)
- 4: UNet (100)
- 8: Text Encoder (1000)
- 16: Adapter (10000)
- 32: Refiner UNet (100000)
- 64: CLIP (1000000)
- 128: å…¶ä»–æ¨¡å— (10000000)

å¸¸è§ç»„åˆç¤ºä¾‹:
- 3: VAE Encoder + Decoder (VAEè®­ç»ƒ)
- 12: UNet + Text Encoder (SDè®­ç»ƒåŸºç¡€)
- 15: VAE + UNet + Text Encoder (å®Œæ•´SDè®­ç»ƒ)
- 31: æ‰€æœ‰SDåŸºç¡€æ¨¡å—
- 128: ESRGANç­‰å…¶ä»–æ¨¡å‹è®­ç»ƒ

TensorBoardä¸­çš„ç›‘æ§æŒ‡æ ‡:
â–  æ¨¡å—çŠ¶æ€å›¾è¡¨:
  - gpu_modules/state_code: æ€»ä½“ç¼–ç æ•°å€¼
  - gpu_modules/module_vae_enc: VAEç¼–ç å™¨çŠ¶æ€ (0/1)
  - gpu_modules/module_vae_dec: VAEè§£ç å™¨çŠ¶æ€ (0/1)
  - gpu_modules/module_unet: UNetçŠ¶æ€ (0/1)
  - gpu_modules/module_text_enc: Text EncoderçŠ¶æ€ (0/1)
  - gpu_modules/module_adapter: AdapterçŠ¶æ€ (0/1)
  - gpu_modules/module_refiner: RefinerçŠ¶æ€ (0/1)
  - gpu_modules/module_clip: CLIPçŠ¶æ€ (0/1)
  - gpu_modules/module_other: å…¶ä»–æ¨¡å—çŠ¶æ€ (0/1)

â–  å†…å­˜ç›‘æ§å›¾è¡¨:
  - gpu_memory/used_mb: GPUå·²ç”¨å†…å­˜(MB)
  - gpu_memory/total_mb: GPUæ€»å†…å­˜(MB)
  - gpu_memory/free_mb: GPUå¯ç”¨å†…å­˜(MB)
  - gpu_memory/utilization: GPUå†…å­˜ä½¿ç”¨ç‡(%)

â–  æ–‡æœ¬æ—¥å¿—:
  - gpu_modules/description: å¯è¯»çš„æ¨¡å—çŠ¶æ€æè¿°

âœ… å®æ—¶ç›‘æ§æ•ˆæœ:
ç°åœ¨æ¯ä¸ªè®­ç»ƒstepéƒ½ä¼šè‡ªåŠ¨è®°å½•GPUæ¨¡å—åŠ è½½çŠ¶æ€ï¼Œä½ å¯ä»¥åœ¨TensorBoardä¸­ï¼š
1. å®æ—¶è§‚å¯Ÿå“ªäº›æ¨¡å—åœ¨GPUä¸Š
2. ç›‘æ§æ¨¡å—åˆ‡æ¢æ¨¡å¼çš„å˜åŒ–
3. è¿½è¸ªGPUå†…å­˜ä½¿ç”¨è¶‹åŠ¿
4. è¯Šæ–­å†…å­˜ä¸è¶³æˆ–æ¨¡å—åŠ è½½é—®é¢˜

ğŸ¯ ä½¿ç”¨åœºæ™¯:
- è°ƒè¯•GPUå†…å­˜ç®¡ç†
- ä¼˜åŒ–æ¨¡å—åŠ è½½ç­–ç•¥
- ç›‘æ§åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æ¨¡å—åˆ†å¸ƒ
- åˆ†æä¸åŒè®­ç»ƒé˜¶æ®µçš„èµ„æºä½¿ç”¨
"""
